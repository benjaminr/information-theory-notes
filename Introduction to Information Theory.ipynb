{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 How can we achieve perfect communication over an imperfect, noisy communication channel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of noisy channels:\n",
    "\n",
    "- Author's Thoughts -> Book -> Reader's Thoughts\n",
    "- Hand Drawing -> Photograph -> Digital Version of Hand Drawing\n",
    "- Musician -> Sound Waves -> Audio Recording\n",
    "\n",
    "There is a probability that the input will not accurately be recoverable or recreated. This is subject to noise (f)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct probability $$1 - f$$ Incorrect probability $$f$$\n",
    "\n",
    "#### A Binary Symmetric Channel\n",
    "x: transmitted\n",
    "y: received\n",
    "f: noise\n",
    "\n",
    "Describing the outcomes in terms of bits:\n",
    "\n",
    "$$P(y=0|x=0) = 1-f; P(y=0|x=1) = f$$\n",
    "$$P(y=1|x=0) = f; P(y=1|x=1) = 1-f$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improvements to Reduce the Probability of Error\n",
    "\n",
    "##### Physical\n",
    "- Improvement the hardware.\n",
    "- More accurate sensors / increased resolution.\n",
    "- Minimise interference / degredation of signal.\n",
    "\n",
    "##### System\n",
    "- Information / Coding theory accepts noisy channels for what they are.\n",
    "- Adds a layer of redundancy for error detection and correction at the receiver.\n",
    "- input source is encoded and transmitted, decoded and received.\n",
    "\n",
    "##### Distinguishing Information Theory vs Coding Theory\n",
    "- Information Theory - What are the limitations of the techniques?\n",
    "- Coding Theory - How do we do it? Encoding / Decoding systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Error-correcting Codes for the Binary Symmetric Channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repition Codes\n",
    "\n",
    "Perhaps the simplest redundancy is the addition of repitition to the transmitted information.\n",
    "\n",
    "A 3 x would be described as $$R_3$$\n",
    "\n",
    "For example, consider the following sequence s for transmission:\n",
    "\n",
    "$$s = 0 0 1 1 0 1 0 1$$\n",
    "\n",
    "Let's define our noise as $$f = 0.3$$\n",
    "\n",
    "We can think of the received message as our transmission (t) plus some noise (n).\n",
    "\n",
    "$$r = t + n$$\n",
    "\n",
    "We can represent our noise as a vector\n",
    "\n",
    "$$t = 000,000,111,111,000,111,000,111$$\n",
    "$$n = 001,001,000,000,100,000,000,000$$\n",
    "$$r = 001,001,111,111,100,111,000,111$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimum Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a majority vote of the received.\n",
    "\n",
    "Calulating using the liklihood ratio:\n",
    "\n",
    "$$\\frac{P(r|s=1)}{P(r|s=0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Bayes theory\n",
    "\n",
    "$$P(s|r_1r_2r_3) = \\frac{P(r_1r_2r_3|s)P(s)}{P(r_1r_2r_3)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can factor in for 1 and 0 cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(1|r_1r_2r_3) = \\frac{P(r_1r_2r_3|1)P(1)}{P(r_1r_2r_3)}$$\n",
    "$$P(0|r_1r_2r_3) = \\frac{P(r_1r_2r_3|0)P(0)}{P(r_1r_2r_3)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is described as the posterior probability and is dependant on two things\n",
    "- prior probability $$P(s)$$\n",
    "- liklihood of s $$P(r_1r_2r_3|s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to calculate the normalising factor, as:\n",
    "- x = 0 $$P(x=0|r)>P(x=1|r)$$\n",
    "- x = 1 $$P(x=1|r)>P(x=0|r)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to make an assumption about the prior probabilities\n",
    "$$P(s=0) = P(s=1) = 0.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the relationship between the posterior probability and the liklihood. If we maxmise for liklihood, that's equivalent to maximising the posterior probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(r|s)=P(r|t(s))=\\prod_{n=1}^{N}P(r_n|t_n(s))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(r_n|t_n) = \\{\\begin{array}{lr}(1-f) if r_n = t_n \\\\ f if r_n \\ne t_n\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The liklihood ratio: $$\\frac{P(r|s=1)}{P(r|s=0)} = \\prod_{n=1}^{N}\\frac{P(r_n|t_n(1))}{P(r_n|t_n(0))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each factor is $$\\frac{1-f}{f} if r_n = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or $$\\frac{f}{1-f} if r_n = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\gamma\\equiv\\frac{(1-f)}{f}$$ This ratio is greater than 1, because f is less than 0.5. So the bit value with the most occurences/ votes wins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst the repetition code has helped reduce the probability of error, it increases the rate of transmission by n, where n is the number of repetitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For odd values of N:\n",
    "\n",
    "$$Pb = \\sum_{n=N+1/n}^N {N \\choose n}f^n(1-f)^{N-n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $$R_3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Pb = \\sum_{n=2}^3 {3 \\choose 2}f^2(1-f)^{3-2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are saying, to get down to the probability of a single bit error, we need to understand the combined error of all possibles instances.\n",
    "\n",
    "To do that, we can sum up all the respective probabilities of the permutations derived from choosing 2 bits from 3.\n",
    "\n",
    "If f=0.1:\n",
    "\n",
    "$$Pb = \\sum_{n=2}^3 {3 \\choose 2}0.1^2(1-0.1)^{3-2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Block Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hamming (7,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we get around encoding each individual bit and minimise compromising our rate. One way is to use a simple block code.\n",
    "\n",
    "Source bits (s) of length K is transmitted (t) with length N\n",
    "\n",
    "The additional bits added are N-K in length and are a linear functions of the input sequence.\n",
    "\n",
    "The name for these bits is parity check bits.\n",
    "\n",
    "$$t_0 t_1 t_2 t_3 p_0(t_0t_1t_2) p_1(t_1t_2t_3) p_2(t_0t_2t_3)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the code is linear, we can represent it in matrix form:\n",
    "\n",
    "Transmitted codewords can be obtained from the source by the following operation:\n",
    "\n",
    "$$G^Ts$$ Where G is the generator matrix.\n",
    "\n",
    "$$\n",
    "G^T =\\begin{bmatrix}\n",
    "    1 & 0 & 0 & 0 \\\\\n",
    "    0 & 1 & 0 & 0 \\\\\n",
    "    0 & 0 & 1 & 0 \\\\\n",
    "    0 & 0 & 0 & 1 \\\\\n",
    "    1 & 1 & 1 & 0 \\\\\n",
    "    0 & 1 & 1 & 1 \\\\\n",
    "    1 & 0 & 1 & 1 \\\\\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoding\n",
    "\n",
    "One way to start this would be to identify the distance of the received from all equiprobable codewords - choosing the one with the minimum distance. A better strategy is something called syndrome decoding.\n",
    "\n",
    "The state of the "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
